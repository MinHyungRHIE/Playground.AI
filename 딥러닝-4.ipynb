{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역전파 학습법\n",
    "\n",
    "## 심층 신경망의 구조\n",
    "\n",
    " - 네트워크 구조를 좀 더 복잡하게 구성\n",
    " - 얕은 신경망(SNN)보다 은닉계층이 많은 신경망을 DNN이라고 부른다.\n",
    " - 보통 5개 이상의 계층이 있는 경우 '깊다'(Deep)라고 표현\n",
    "\n",
    "## 무엇이 다를까\n",
    " - 은닉 계층 추가 = 특징의 비선형 변환 추가\n",
    " - 학습 매개변수의 수가 계층 크기의 제곱에 비례\n",
    " - Sigmoid 활성 함수 동작이 원할하지 않음\n",
    "   - ReLu(Rectified Linear Unit)도입 필요\n",
    "\n",
    "## 역전파 학습법의 개념\n",
    " - 학습 환경이 주어졌을 때, 손실 함수를 매개 변수로 여러 번 미분해야 한다.\n",
    " - 의존성이 있는 함수의 계산\n",
    "    - 동일 연산이 2회 필요하므로, 중복되는 계산이 1회 발생한다.\n",
    " - 의존성이 있는 함수의 계산의 문제를 해결하기 위해 : 동적 계획법(Dynamic Programming)\n",
    "    - 처음 계산될 때 값은 한번 저장, 첫 계산시 값을 저장하므로 중복 계산이 발생하지 않는다.\n",
    "    - 하게될 미분 연산은 동일한 연산 값을 여러번 참조해야하기 때문에 동적 계획법이 효율적\n",
    " - 연쇄 법칙 Chain Rule\n",
    "    - 연속된 두 함수의 미분은, 각 함수의 미분을 연쇄적으로 곱한 것과 같다.\n",
    " - 출력 계층의 미분 : 연쇄 법칙을 이용하려면 손실 함수의 미분이 필요하다\n",
    " - 마지막 은닉 계층의 미분 : 연쇄 법칙을 이용하려면 손실함수, 출력계층의 미분이 필요하다. 출력 계층, 손실함수의 미분을 저장해 두면(동적계획법) 중복 연산을 피할 수 있다.\n",
    " - 연쇄 법칙을 이용하려면 손실함수, 출력계층, 사이의 모든 은닉계층의 미분이 필요하다.\n",
    " \n",
    "### 순방향 추론 Forward Inference\n",
    " - 현재 매개변수에서 손실 값을 계산하기 위해 순차적ㅇ니 연산을 수행하는 것을 순방향 추론이라 한다.\n",
    " - 학습을 마친 후 알고리즘을 사용할 때는 순방향 추론을 사용한다.\n",
    "\n",
    "### 역전파 학습법 Back-Propagation\n",
    " - 심층 신경망의 미분을 계산하기 위해, 연쇄 법칙과 동적 계획법을 이용하여 효율적으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파 학습의 필요성\n",
    " - 블랙박스 모델의 학습\n",
    "    - 매개변수에 따라 동작이 달라진다.\n",
    " - 수치적 기울기 Numerical Gradient : 미분의 정의로부터 극한 연산을 근사해 수치적 기울기를 구할 수 있다.\n",
    " - 블랙박스 모델의 수치적 기울기\n",
    "    - 각 스칼라 변수를 각각 조금씩 바꾸어 대입해 보면서 수치적 기울기를 구한다.\n",
    " - 심층 신경망의 수치적 기울기\n",
    "    - 10만개 파라미터를 가진 경우 무려 100억회를 곱해야한다. \n",
    "    - 이를 해결하기 위해 역전파 학습을 한다\n",
    "\n",
    "## 합성 함수와 연쇄법칙\n",
    " - 연쇄 법칙을 이용하면 연속된 함수의 미분을 각각의 미분의 곱으로 표현할 수 있다.\n",
    " - f+n(h_n-1) = a(W_nh_n-1 _ b_n)\n",
    "\n",
    "### 이미 손실을 구했다면, 데이터의 입력과 출력은 학습 과정에서 중요하지 않다.\n",
    " - 손실을 최소화하는 파라미터만 찾으면 되기 때문!\n",
    " - 미분하고자하는 경로사이에 있는 모든 미분 값을 곱하면 원하는 미분을 구할 수 있다.\n",
    "    - 즉, 원하는 미분을 얻으려면 경로 사이에 있는 모든 미분 값을 다 알아야 한다는 말이다.\n",
    "    \n",
    "### Sigmoid 함수의 미분\n",
    " - 초창기 신경망에 가장 많이 쓰인 Sigmoid 활성 함수의 미분. 정리된 결과를 이용하면 매우 간단하게 미분할 수 있다.\n",
    " \n",
    "## 역전파 알고리즘\n",
    " - 오류 역전파 알고리즘(Back-Propagation Algorithm; BP)\n",
    " - 정방향 연산 시, 계측별로 BP에 필요한 중간 결과를 저장한다.\n",
    " - Loos를 각 파라미터로 미분한다. 연쇄법칙(역방향 연산)을 이용한다.\n",
    " - 미분의 연쇄 법칙과 각 함수의 수식적 미분을 이용하면, 단 한번의 손실 함수 평가로 미분을 구할 수 있다. 단, 중간 결과를 저장해야 하므로 메모리를 추가로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수치 미분을 이용한 심층 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_error(h, y):\n",
    "    return 1 / 2 * np.mean(np.square(h - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴런 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, W, b, a):\n",
    "        # Model Parameter\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W), x) + self.b) # activation((W^T)x + b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심층 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    # hidden_depth(히든 레이어 갯수), num_neuron(히든 레이어 하나당 뉴런이 몇개 있는지) \n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "        \n",
    "        self.sequence = list()\n",
    "        #First hidden layer\n",
    "        W, b = init_var(num_input, num_neuron)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "        #Hidden layers\n",
    "        for _ in range(hidden_depth - 1):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "        #Output layer\n",
    "        W, b = init_var(num_neuron, num_output)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_gradient(self, x, y, loss_func): # x(추정), y(정답)\n",
    "        def get_new_sequence(layer_index, new_neuron):\n",
    "            new_sequence = list()\n",
    "            for i, layer in enumerate(self.sequence):\n",
    "                if i == layer_index:\n",
    "                    new_sequence.append(new_neuron)\n",
    "                else:\n",
    "                    new_sequence.append(layer)\n",
    "            return new_sequence\n",
    "        \n",
    "        def eval_sequence(x, sequence):\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "        loss = loss_func(self(x), y)\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.sequence): # iterate layer\n",
    "            for w_i, w in enumerate(layer.W): #iterate W (row)\n",
    "                for w_j, ww in enumerate(w): #iterate W (col)\n",
    "                    W = np.copy(layer.W)\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "                    \n",
    "                    new_neuron = Neuron(W, layer.b, layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_neuron)\n",
    "                    h = eval_sequence(x, new_seq)\n",
    "                    \n",
    "                    num_grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon\n",
    "                    layer.dW[w_i][w_j] = num_grad\n",
    "                    \n",
    "                for b_i, bb in enumerate(layer.b): # iterate b\n",
    "                    b = np.copy(layer.b)\n",
    "                    b[b_i] = bb + epsilon\n",
    "                    \n",
    "                    new_neuron = Neuron(layer.W, b, layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_neuron)\n",
    "                    h = eval_sequence(x, new_seq)\n",
    "                    \n",
    "                    num_grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon\n",
    "                    layer.db[b_i] = num_grad\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강 학습법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = network.calc_gradient(x, y, loss_obj)\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Test loss 0.27381181131232146\n",
      "Epoch 1: Test loss 0.2722886780345872\n",
      "Epoch 2: Test loss 0.2707744287998881\n",
      "Epoch 3: Test loss 0.2692691172362483\n",
      "Epoch 4: Test loss 0.26777279378142216\n",
      "Epoch 5: Test loss 0.2662855057134419\n",
      "Epoch 6: Test loss 0.26480729718330176\n",
      "Epoch 7: Test loss 0.26333820924968077\n",
      "Epoch 8: Test loss 0.26187827991572454\n",
      "Epoch 9: Test loss 0.26042754416752173\n",
      "Epoch 10: Test loss 0.2589860340144733\n",
      "Epoch 11: Test loss 0.257553778531255\n",
      "Epoch 12: Test loss 0.2561308039013556\n",
      "Epoch 13: Test loss 0.254717133462038\n",
      "Epoch 14: Test loss 0.25331278775061383\n",
      "Epoch 15: Test loss 0.2519177845520455\n",
      "Epoch 16: Test loss 0.25053213894760185\n",
      "Epoch 17: Test loss 0.2491558633646338\n",
      "Epoch 18: Test loss 0.24778896762722719\n",
      "Epoch 19: Test loss 0.24643145900780505\n",
      "Epoch 20: Test loss 0.24508334227939182\n",
      "Epoch 21: Test loss 0.2437446197686337\n",
      "Epoch 22: Test loss 0.24241529140932716\n",
      "Epoch 23: Test loss 0.2410953547965326\n",
      "Epoch 24: Test loss 0.2397848052410272\n",
      "Epoch 25: Test loss 0.23848363582412635\n",
      "Epoch 26: Test loss 0.2371918374527487\n",
      "Epoch 27: Test loss 0.2359093989146897\n",
      "Epoch 28: Test loss 0.23463630693395987\n",
      "Epoch 29: Test loss 0.23337254622618453\n",
      "Epoch 30: Test loss 0.2321180995539539\n",
      "Epoch 31: Test loss 0.2308729477821227\n",
      "Epoch 32: Test loss 0.22963706993286898\n",
      "Epoch 33: Test loss 0.2284104432406152\n",
      "Epoch 34: Test loss 0.2271930432066383\n",
      "Epoch 35: Test loss 0.22598484365335822\n",
      "Epoch 36: Test loss 0.22478581677822304\n",
      "Epoch 37: Test loss 0.22359593320722884\n",
      "Epoch 38: Test loss 0.2224151620479073\n",
      "Epoch 39: Test loss 0.22124347094180807\n",
      "Epoch 40: Test loss 0.22008082611647062\n",
      "Epoch 41: Test loss 0.21892719243674746\n",
      "Epoch 42: Test loss 0.2177825334555386\n",
      "Epoch 43: Test loss 0.2166468114638508\n",
      "Epoch 44: Test loss 0.21551998754018062\n",
      "Epoch 45: Test loss 0.21440202159915073\n",
      "Epoch 46: Test loss 0.2132928724394595\n",
      "Epoch 47: Test loss 0.2121924977909935\n",
      "Epoch 48: Test loss 0.21110085436124354\n",
      "Epoch 49: Test loss 0.2100178978808276\n",
      "Epoch 50: Test loss 0.2089435831482478\n",
      "Epoch 51: Test loss 0.20787786407381728\n",
      "Epoch 52: Test loss 0.20682069372269277\n",
      "Epoch 53: Test loss 0.20577202435710865\n",
      "Epoch 54: Test loss 0.2047318074776801\n",
      "Epoch 55: Test loss 0.20369999386390458\n",
      "Epoch 56: Test loss 0.20267653361370785\n",
      "Epoch 57: Test loss 0.20166137618216515\n",
      "Epoch 58: Test loss 0.20065447041929485\n",
      "Epoch 59: Test loss 0.19965576460698242\n",
      "Epoch 60: Test loss 0.19866520649500352\n",
      "Epoch 61: Test loss 0.1976827433361869\n",
      "Epoch 62: Test loss 0.19670832192062787\n",
      "Epoch 63: Test loss 0.19574188860910466\n",
      "Epoch 64: Test loss 0.1947833893655453\n",
      "Epoch 65: Test loss 0.1938327697886434\n",
      "Epoch 66: Test loss 0.19288997514263048\n",
      "Epoch 67: Test loss 0.19195495038713828\n",
      "Epoch 68: Test loss 0.19102764020626556\n",
      "Epoch 69: Test loss 0.19010798903676024\n",
      "Epoch 70: Test loss 0.18919594109538213\n",
      "Epoch 71: Test loss 0.1882914404054265\n",
      "Epoch 72: Test loss 0.18739443082247828\n",
      "Epoch 73: Test loss 0.186504856059291\n",
      "Epoch 74: Test loss 0.18562265970994246\n",
      "Epoch 75: Test loss 0.18474778527316055\n",
      "Epoch 76: Test loss 0.18388017617492478\n",
      "Epoch 77: Test loss 0.18301977579026235\n",
      "Epoch 78: Test loss 0.18216652746435213\n",
      "Epoch 79: Test loss 0.18132037453286365\n",
      "Epoch 80: Test loss 0.18048126034160283\n",
      "Epoch 81: Test loss 0.17964912826544754\n",
      "Epoch 82: Test loss 0.17882392172658115\n",
      "Epoch 83: Test loss 0.17800558421207174\n",
      "Epoch 84: Test loss 0.17719405929078877\n",
      "Epoch 85: Test loss 0.176389290629667\n",
      "Epoch 86: Test loss 0.1755912220093341\n",
      "Epoch 87: Test loss 0.17479979733912954\n",
      "Epoch 88: Test loss 0.17401496067151329\n",
      "Epoch 89: Test loss 0.17323665621588857\n"
     ]
    }
   ],
   "source": [
    "# 임의의 데이터 셋\n",
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
