{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역전파 학습법\n",
    "\n",
    "## 심층 신경망의 구조\n",
    "\n",
    " - 네트워크 구조를 좀 더 복잡하게 구성\n",
    " - 얕은 신경망(SNN)보다 은닉계층이 많은 신경망을 DNN이라고 부른다.\n",
    " - 보통 5개 이상의 계층이 있는 경우 '깊다'(Deep)라고 표현\n",
    "\n",
    "## 무엇이 다를까\n",
    " - 은닉 계층 추가 = 특징의 비선형 변환 추가\n",
    " - 학습 매개변수의 수가 계층 크기의 제곱에 비례\n",
    " - Sigmoid 활성 함수 동작이 원할하지 않음\n",
    "   - ReLu(Rectified Linear Unit)도입 필요\n",
    "\n",
    "## 역전파 학습법의 개념\n",
    " - 학습 환경이 주어졌을 때, 손실 함수를 매개 변수로 여러 번 미분해야 한다.\n",
    " - 의존성이 있는 함수의 계산\n",
    "    - 동일 연산이 2회 필요하므로, 중복되는 계산이 1회 발생한다.\n",
    " - 의존성이 있는 함수의 계산의 문제를 해결하기 위해 : 동적 계획법(Dynamic Programming)\n",
    "    - 처음 계산될 때 값은 한번 저장, 첫 계산시 값을 저장하므로 중복 계산이 발생하지 않는다.\n",
    "    - 하게될 미분 연산은 동일한 연산 값을 여러번 참조해야하기 때문에 동적 계획법이 효율적\n",
    " - 연쇄 법칙 Chain Rule\n",
    "    - 연속된 두 함수의 미분은, 각 함수의 미분을 연쇄적으로 곱한 것과 같다.\n",
    " - 출력 계층의 미분 : 연쇄 법칙을 이용하려면 손실 함수의 미분이 필요하다\n",
    " - 마지막 은닉 계층의 미분 : 연쇄 법칙을 이용하려면 손실함수, 출력계층의 미분이 필요하다. 출력 계층, 손실함수의 미분을 저장해 두면(동적계획법) 중복 연산을 피할 수 있다.\n",
    " - 연쇄 법칙을 이용하려면 손실함수, 출력계층, 사이의 모든 은닉계층의 미분이 필요하다.\n",
    " \n",
    "### 순방향 추론 Forward Inference\n",
    " - 현재 매개변수에서 손실 값을 계산하기 위해 순차적ㅇ니 연산을 수행하는 것을 순방향 추론이라 한다.\n",
    " - 학습을 마친 후 알고리즘을 사용할 때는 순방향 추론을 사용한다.\n",
    "\n",
    "### 역전파 학습법 Back-Propagation\n",
    " - 심층 신경망의 미분을 계산하기 위해, 연쇄 법칙과 동적 계획법을 이용하여 효율적으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파 학습의 필요성\n",
    " - 블랙박스 모델의 학습\n",
    "    - 매개변수에 따라 동작이 달라진다.\n",
    " - 수치적 기울기 Numerical Gradient : 미분의 정의로부터 극한 연산을 근사해 수치적 기울기를 구할 수 있다.\n",
    " - 블랙박스 모델의 수치적 기울기\n",
    "    - 각 스칼라 변수를 각각 조금씩 바꾸어 대입해 보면서 수치적 기울기를 구한다.\n",
    " - 심층 신경망의 수치적 기울기\n",
    "    - 10만개 파라미터를 가진 경우 무려 100억회를 곱해야한다. \n",
    "    - 이를 해결하기 위해 역전파 학습을 한다\n",
    "\n",
    "## 합성 함수와 연쇄법칙\n",
    " - 연쇄 법칙을 이용하면 연속된 함수의 미분을 각각의 미분의 곱으로 표현할 수 있다.\n",
    " - f+n(h_n-1) = a(W_nh_n-1 _ b_n)\n",
    "\n",
    "### 이미 손실을 구했다면, 데이터의 입력과 출력은 학습 과정에서 중요하지 않다.\n",
    " - 손실을 최소화하는 파라미터만 찾으면 되기 때문!\n",
    " - 미분하고자하는 경로사이에 있는 모든 미분 값을 곱하면 원하는 미분을 구할 수 있다.\n",
    "    - 즉, 원하는 미분을 얻으려면 경로 사이에 있는 모든 미분 값을 다 알아야 한다는 말이다.\n",
    "    \n",
    "### Sigmoid 함수의 미분\n",
    " - 초창기 신경망에 가장 많이 쓰인 Sigmoid 활성 함수의 미분. 정리된 결과를 이용하면 매우 간단하게 미분할 수 있다.\n",
    " \n",
    "## 역전파 알고리즘\n",
    " - 오류 역전파 알고리즘(Back-Propagation Algorithm; BP)\n",
    " - 정방향 연산 시, 계츠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
