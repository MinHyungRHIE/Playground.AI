{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`인공 지능` 기계가 사람의 행동을 모방하게 하는 기술\n",
    " - `기계 학습` 기계가 일일이 코드로 명시하지 않은 동ㅈ가을 데이터로부터 학습하여 실행할 수 있도록 하는 알고리즘을 개발하는 분야\n",
    "  - `딥러닝` 기계학습의 한 분야인 인공 신경망에 기반하여, 많은 양의 데이터를 학습해 뛰어난 성능을 이끌에내는 연구 분야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `빅데이터`\n",
    "    - 데이터베이스 관리\n",
    "    - 데이터 저장/유통\n",
    "    - 데이터 수집\n",
    "    - 데이터 신뢰성 확보\n",
    "    - 데이터 시각화\n",
    "    - 데이터 통계 분석\n",
    "    - 데이터 마이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 기계학습(Machine Learning)\n",
    "  - Data -> 특징 추출기(Feature Extractor) -> 특징 벡터(Feature Vector) -> 분류기(Classifier):학습대상 -> 예측\n",
    " \n",
    " - 딥러닝(Deep Learning)\n",
    "  - Data -> Feature Extractor & Classifier : 학습대상 -> 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝으로 무엇을 할 수 있는가?\n",
    " - 분류 Classification\n",
    " - 회귀 Regression\n",
    " - 물체 검출 Object Detection\n",
    " - 영상 분할 Image Segmentation\n",
    " - 영상 초해상도 Image Super Resolution\n",
    " - 예술적 창조물 Artistic Creation with GAN\n",
    " - 강화 학습 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 구성 요소\n",
    " - 학습 단계 (Training Phase)\n",
    "    - 학습 데이터셋(Training Dataset)\n",
    "      - 학습 입력(Training Input), 학습 정답(Training Groundtruth)\n",
    "    - 학습 입력->네트워크 구조(Network Architecture)\n",
    "    - 학습 정답->손실 함수(Loss Function)\n",
    "    - 알고리즘 최적화 기법(Algorithm Optimizer) -> 네트워크 구조[반복]\n",
    "\n",
    " - 테스트 단계(Test Phase)\n",
    "    - 테스트 입력(Test Input)->학습된 네트워크(Trained Network)->평가지표(Evaluation Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 역사\n",
    " 1. 1943 : Eletronic Brain - 최초의 인공신경망 개념 McCulloch and Pitts\n",
    " 2. 1957 : Perceptron - Rosenblatt의 퍼셉트론\n",
    " 3. 1960 : ADALINE \n",
    "    - 1960 ~ 1969 : Golden Age\n",
    " 4. 1969 : XOR Problem - 퍼셉트론의 한계(Minky and Papert, 1969 - Perceptrons)\n",
    "    - 1969 ~ 1986 : Dark Age(\"AI Winter\") \"XOR도 못푸냐\"\n",
    " 5. 1986 : Multi-layered Perceptron(Backpropagation)\n",
    "    - 다층 퍼셉트론(XOR 문제 해결, 선 2개로) : 입력 계층 - 은닉계층 - 출력 계층;MLP\n",
    "    - 오류 역전파 알고리즘(Backpropagation Algorithm;BP\n",
    "    - 두번째 AI Winter\n",
    "     - 기울기 소실 문제 : 은닉 계층이 깊을 수록(Deep Neural Network) 학습이 어려운 기울기 소실(Vanishing Gradient) 문제\n",
    " 6. 1995 : SVM\n",
    " 7. 2006 : Deep Neural Network(Pretraining) - 심층 믿음 신경망\n",
    "    - RBM, Restricted Boltzmann Machine 비지도 학습법 : 입력 데이터만으로 학습하는 방법\n",
    "    - RBM을 쌓아 올린 DBN(Deep Belief Network, Hinton, 2006)\n",
    " 8. ImageNet Large Scale Visual Recognition Challenges\n",
    "    - AlexNet이 2012 우승과 함께 딥러닝 급부상\n",
    "    \n",
    "   \n",
    "| 구분 | 이름 | 오류 |\n",
    "|:---:|:---:|:---:|\n",
    "|2010|NEC-UIUC|28%|\n",
    "|2011|XRCE|26%|\n",
    "|2012|AlexNet|16%|\n",
    "|2013|ZFNet|12%|\n",
    "|2014|GoogLeNet, VGGNet|7%|\n",
    "|인간| |5%|\n",
    "|2015|ResNet|3.6%|\n",
    "|2016|GoogLeNet-v4|3%|\n",
    "|2017|SENet|2.3%|\n",
    "|2018|DenseNet||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝의 현재\n",
    " - 딥러닝 이전에는 서포트 벡터 머신이 왕좌\n",
    " - AlexNet이 나오면서 딥러닝 대중화\n",
    "   - 초창기 Framework\n",
    "     - Caffe2 : 리눅스 기반\n",
    "     - MatConvNet : 윈도우 기반, MATLAB 환경에 익숙한 연구원\n",
    "     - 둘 다 NVIDIA의 CUDA를 사용\n",
    "   - 현재 Framework\n",
    "     - TensorFlow : 다양한 플랫폼\n",
    "     - PyTorch : 속도가 빠르고 진입장벽이 낮음\n",
    "   - Cloud Platform\n",
    "     - AWS\n",
    "     - Google Cloud Platform\n",
    "     - Microsoft Azure\n",
    "     \n",
    "![클라우드 플랫폼](./mdsrc/Cloud_Platform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy hyperparameters\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='sigmoid')\n",
    "        self.d2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training loop\n",
    "@tf.function\n",
    "def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement algorithm test\n",
    "@tf.function\n",
    "def test_step(model, images, labels, loss_object, test_loss, test_accuracy):\n",
    "    predictions = model(images)\n",
    "    \n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 가져오고, 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and organize dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 생성 (모델 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  손실 함수, 최적화 알고리즘 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  알고리즘 평가지표 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘 학습, 테스트 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 0.40371203422546387, Accuracy: 89.47833251953125, Test Loss: 0.2271452099084854, Test Accuracy: 93.61000061035156\n",
      "Epoch 2, Loss: 0.3004540503025055, Accuracy: 91.8933334350586, Test Loss: 0.19567258656024933, Test Accuracy: 94.45499420166016\n",
      "Epoch 3, Loss: 0.24825021624565125, Accuracy: 93.22222137451172, Test Loss: 0.1748344451189041, Test Accuracy: 94.94000244140625\n",
      "Epoch 4, Loss: 0.21401314437389374, Accuracy: 94.11666107177734, Test Loss: 0.15911546349525452, Test Accuracy: 95.3800048828125\n",
      "Epoch 5, Loss: 0.1890459805727005, Accuracy: 94.79033660888672, Test Loss: 0.14676900207996368, Test Accuracy: 95.70199584960938\n"
     ]
    }
   ],
   "source": [
    "# Do training loop and test\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "        \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy)\n",
    "    \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(\n",
    "    epoch + 1,\n",
    "    train_loss.result(),\n",
    "    train_accuracy.result() * 100,\n",
    "    test_loss.result(),\n",
    "    test_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
