{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역전파 학습법\n",
    "\n",
    "## 심층 신경망의 구조\n",
    "\n",
    " - 네트워크 구조를 좀 더 복잡하게 구성\n",
    " - 얕은 신경망(SNN)보다 은닉계층이 많은 신경망을 DNN이라고 부른다.\n",
    " - 보통 5개 이상의 계층이 있는 경우 '깊다'(Deep)라고 표현\n",
    "\n",
    "## 무엇이 다를까\n",
    " - 은닉 계층 추가 = 특징의 비선형 변환 추가\n",
    " - 학습 매개변수의 수가 계층 크기의 제곱에 비례\n",
    " - Sigmoid 활성 함수 동작이 원할하지 않음\n",
    "   - ReLu(Rectified Linear Unit)도입 필요\n",
    "\n",
    "## 역전파 학습법의 개념\n",
    " - 학습 환경이 주어졌을 때, 손실 함수를 매개 변수로 여러 번 미분해야 한다.\n",
    " - 의존성이 있는 함수의 계산\n",
    "    - 동일 연산이 2회 필요하므로, 중복되는 계산이 1회 발생한다.\n",
    " - 의존성이 있는 함수의 계산의 문제를 해결하기 위해 : 동적 계획법(Dynamic Programming)\n",
    "    - 처음 계산될 때 값은 한번 저장, 첫 계산시 값을 저장하므로 중복 계산이 발생하지 않는다.\n",
    "    - 하게될 미분 연산은 동일한 연산 값을 여러번 참조해야하기 때문에 동적 계획법이 효율적\n",
    " - 연쇄 법칙 Chain Rule\n",
    "    - 연속된 두 함수의 미분은, 각 함수의 미분을 연쇄적으로 곱한 것과 같다.\n",
    " - 출력 계층의 미분 : 연쇄 법칙을 이용하려면 손실 함수의 미분이 필요하다\n",
    " - 마지막 은닉 계층의 미분 : 연쇄 법칙을 이용하려면 손실함수, 출력계층의 미분이 필요하다. 출력 계층, 손실함수의 미분을 저장해 두면(동적계획법) 중복 연산을 피할 수 있다.\n",
    " - 연쇄 법칙을 이용하려면 손실함수, 출력계층, 사이의 모든 은닉계층의 미분이 필요하다.\n",
    " \n",
    "### 순방향 추론 Forward Inference\n",
    " - 현재 매개변수에서 손실 값을 계산하기 위해 순차적ㅇ니 연산을 수행하는 것을 순방향 추론이라 한다.\n",
    " - 학습을 마친 후 알고리즘을 사용할 때는 순방향 추론을 사용한다.\n",
    "\n",
    "### 역전파 학습법 Back-Propagation\n",
    " - 심층 신경망의 미분을 계산하기 위해, 연쇄 법칙과 동적 계획법을 이용하여 효율적으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파 학습의 필요성\n",
    " - 블랙박스 모델의 학습\n",
    "    - 매개변수에 따라 동작이 달라진다.\n",
    " - 수치적 기울기 Numerical Gradient : 미분의 정의로부터 극한 연산을 근사해 수치적 기울기를 구할 수 있다.\n",
    " - 블랙박스 모델의 수치적 기울기\n",
    "    - 각 스칼라 변수를 각각 조금씩 바꾸어 대입해 보면서 수치적 기울기를 구한다.\n",
    " - 심층 신경망의 수치적 기울기\n",
    "    - 10만개 파라미터를 가진 경우 무려 100억회를 곱해야한다. \n",
    "    - 이를 해결하기 위해 역전파 학습을 한다\n",
    "\n",
    "## 합성 함수와 연쇄법칙\n",
    " - 연쇄 법칙을 이용하면 연속된 함수의 미분을 각각의 미분의 곱으로 표현할 수 있다.\n",
    " - f+n(h_n-1) = a(W_nh_n-1 _ b_n)\n",
    "\n",
    "### 이미 손실을 구했다면, 데이터의 입력과 출력은 학습 과정에서 중요하지 않다.\n",
    " - 손실을 최소화하는 파라미터만 찾으면 되기 때문!\n",
    " - 미분하고자하는 경로사이에 있는 모든 미분 값을 곱하면 원하는 미분을 구할 수 있다.\n",
    "    - 즉, 원하는 미분을 얻으려면 경로 사이에 있는 모든 미분 값을 다 알아야 한다는 말이다.\n",
    "    \n",
    "### Sigmoid 함수의 미분\n",
    " - 초창기 신경망에 가장 많이 쓰인 Sigmoid 활성 함수의 미분. 정리된 결과를 이용하면 매우 간단하게 미분할 수 있다.\n",
    " \n",
    "## 역전파 알고리즘\n",
    " - 오류 역전파 알고리즘(Back-Propagation Algorithm; BP)\n",
    " - 정방향 연산 시, 계측별로 BP에 필요한 중간 결과를 저장한다.\n",
    " - Loos를 각 파라미터로 미분한다. 연쇄법칙(역방향 연산)을 이용한다.\n",
    " - 미분의 연쇄 법칙과 각 함수의 수식적 미분을 이용하면, 단 한번의 손실 함수 평가로 미분을 구할 수 있다. 단, 중간 결과를 저장해야 하므로 메모리를 추가로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수치 미분을 이용한 심층 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "\n",
    "def _t(x):\n",
    "    return np.transpose(x)\n",
    "\n",
    "def _m(A, B):\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def mean_squared_error(h, y):\n",
    "    return 1 / 2 * np.mean(np.square(h - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴런 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, W, b, a):\n",
    "        # Model Parameter\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.a = a\n",
    "        \n",
    "        # Gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a(_m(_t(self.W), x) + self.b) # activation((W^T)x + b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 심층 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    # hidden_depth(히든 레이어 갯수), num_neuron(히든 레이어 하나당 뉴런이 몇개 있는지) \n",
    "    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):\n",
    "        def init_var(i, o):\n",
    "            return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))\n",
    "        self.sequence = list()\n",
    "        #First hidden layer\n",
    "        W, b = init_var(num_input, num_neuron)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "        #Hidden layers\n",
    "        for _ in range(hidden_depth - 1):\n",
    "            W, b = init_var(num_neuron, num_neuron)\n",
    "            self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "        #Output layer\n",
    "        W, b = init_var(num_neuron, num_output)\n",
    "        self.sequence.append(Neuron(W, b, activation))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.sequence:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calc_gradient(self, x, y, loss_func): # x(추정), y(정답)\n",
    "        def get_new_sequence(layer_index, new_neuron):\n",
    "            new_sequence = list()\n",
    "            for i, l in enumerate(self.sequence):\n",
    "                if i == layer_index:\n",
    "                    new_sequence.append(new_neuron)\n",
    "                else:\n",
    "                    new_sequence.append(layer)\n",
    "            return new_sequence\n",
    "        \n",
    "        def eval_sequence(x, sequence):\n",
    "            for layer in sequence:\n",
    "                x = layer(x)\n",
    "            return x\n",
    "        \n",
    "        loss = loss_func(self.__init__(x), y)\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.sequence): # iterate layer\n",
    "            for w_i, w in enumerate(layer.W): #iterate W (row)\n",
    "                for w_j, ww in enumerate(w): #iterate W (col)\n",
    "                    W = np.copy(layer.W)\n",
    "                    W[w_i][w_j] = ww + epsilon\n",
    "                    \n",
    "                    new_neuron = Neuron(W, layer.b, layer.a)\n",
    "                    new_seq = get_new_sequence(layer_id, new_neuron)\n",
    "                    h = eval_seq(x, new_seq)\n",
    "                    \n",
    "                    grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강 학습법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(network, x, y, loss_obj, alpha=0.01):\n",
    "    loss = network.calc_gradient(x, y, loss_obj)\n",
    "    for layer in network.sequence:\n",
    "        layer.W += -alpha * layer.dW\n",
    "        layer.b += -alpha * layer.db\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-62538e529c7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_neuron\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivatio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DNN' is not defined"
     ]
    }
   ],
   "source": [
    "# 임의의 데이터 셋\n",
    "x = np.random.normal(0.0, 1.0, (10,))\n",
    "y = np.random.normal(0.0, 1.0, (2,))\n",
    "\n",
    "dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activatio=sigmoid)\n",
    "\n",
    "t = time.time()\n",
    "for epoch in range(100):\n",
    "    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)\n",
    "    print('Epoch {}: Test loss {}'.format(epoch, loss))\n",
    "print('{} seconds elapsed.'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
