{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순환 신경망(RNN)\n",
    "\n",
    "## 순차 데이터의 이해\n",
    "\n",
    "### 순차 데이터 sequential data\n",
    "\n",
    " - 순서가 의미가 있으며, 순서가 달라질 경우 의미가 손상되는 데이터를 순차 데이터라고 한다.\n",
    " - 시간적 의미가 있을 경우 Temporal Seqence라고 하며, 일정한 시간차라면 Time Series라고 한다.\n",
    " \n",
    "    - DNA 염기 서열 : Sequential Data\n",
    "    - 세계 기온 변화 : Temporal Seqence\n",
    "    - 샘플링된 소리 신호 : Time Series\n",
    "    \n",
    "### Resampling\n",
    " - 데이터를 편하게 다루기 위해서는 Time Series 형태로 있는 것이 좋다.\n",
    " - 그러나 데이터 취득할 때 Temporal Seqence로 받는 경우가 많다.\n",
    " - Temporal Seqence를 --> Time Series로 변환하기 위해서는 Resample을 수행한다.\n",
    " - 취득된 데이터(Temporal Sequence)를 이용해 신호를 보간(Interpolation)하고, 이를 균일 시간 간격으로 샘플링 한다.\n",
    " \n",
    "### 심층 신경망과 순차 데이터\n",
    " - 심층 신경망으로 음성 인식을 하려고 할 때, 입출력 모두 문제가 있다. 초당 44k, 4만 4천개...?\n",
    "   - 입력되는 음성의 길이는 매번 다름\n",
    " \n",
    "### 다중 입력, 단일 출력\n",
    " - 1. What, 2. time, 3. is, 4. it\n",
    " - Seqence Data Processor\n",
    " - \"It's {TIME}.\"\n",
    " - 개인 비서 서비스는 다중 입력에 대해 단일 출력하는 모델\n",
    " \n",
    "### 다중 입력, 다중 출력\n",
    " - 번역기는 다중 입력에 대해 다중 출력을 해주는 모델이다. 물론 입력과 출력의 길이는 다를 수 있다.\n",
    " \n",
    "### 단일 입력, 다중 출력\n",
    " - 사진을 묘사하는 장면 이해 알고리즘은 단일 입력에 대해 다중 출력을 내는 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본적인 순환 신경망\n",
    "\n",
    "### 기억 시스템 Memory System\n",
    " - Sequence Data Processor[기억] -> SDP[기억] ...\n",
    " - 올바른 대답을 하려면, 입력을 받을 때마다 그 내용을 기억할 수 있어야 한다.\n",
    " - 이전 입력을 기억하지 않은 시스템은 무기억 시스템(Memoryless System)\n",
    "   - Shallow Neural Network는 대표적인 무기억 시스템\n",
    "   \n",
    "### 기본적인 순환 신경망 Vanilla Recurrent Network\n",
    " - Vanilla RNN 구조는 얕은 신경망 구조에 '순환'이 추가된 것으로 이해할 수 있다.\n",
    " - 기억 시스템이므로 RNN 출력은 이전 모든 입력에 영향을 받는다.\n",
    " \n",
    "### 다중 계층 순환 신경망 Multi-Layer RNN\n",
    " - 순환 신경망도 심층 신경망 처럼 쌓아 올릴 수 있다.\n",
    " - 하지만 신경망의 구조가 매우 복잡해지고 학습이 잘 되지 않아 권장되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 심화 순환 신경망\n",
    "\n",
    "### 순환 신경망 Recurrent Neural Network\n",
    "\n",
    "### 왜 Vanilla RNN을 잘 쓰지 않은가\n",
    " - 기울기 소실 문제가 있음\n",
    " - 어떤 입력의 정보가 사용되는 시점이 차이가 많이 날 경우, 학습 능력이 저하된다.\n",
    "\n",
    "### LSTM : Long Short-Term Memory\n",
    " - Vanilla RNN을 개선한 LSTM 구조, 조금 복잡해 보이지만 기억할 것을 오래 기억하고 잊을 것은 빨리 잊어버리는 능력이 있다.\n",
    " - Cell State가 추가됨\n",
    "    - Cell State - 기억을 오랫동안 유지할 수 있는 구조, 새로운 특징을 덧셈으로 받는 구조 ( Residual Network\n",
    "    - Hidden State - 계층의 출력/다음 타임 스텝으로 넘기는 정보\n",
    " - RNN과 달리, Cell State가 있어서 '기억'에 관한 부분을 전담한다\n",
    " - Forget Gate - Sigmoid 활성 함수로 0-~1 출력 값을 가짐\n",
    "    - Cell State에 이를 곱해주어서 '얼만큼 잊을지'를 결정\n",
    " - Input Gate\n",
    "   - 새롭게 추출한 특징을 얼만큼 사용할 지 결정\n",
    " - Output Gate\n",
    "   - Cell로부터 출력을 얼마나 내보낼지 결정하는 역할\n",
    "\n",
    "### GRU : Gated Recurrent Unit\n",
    " - Cell State가 없고, Hidden State만 존재한다.\n",
    " - Forget Gate와 Input Gate를 결합\n",
    " - Reset Gate가 추가됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
